{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning II: Deep learning classifiers for urban sound data**\n",
    "<img src=\"images/portrait.png\" alt=\"Portrait\" style=\"width:100%; height:auto;\">\n",
    "\n",
    "===================================================================================================================\n",
    "\n",
    "\n",
    "## **Introduction**\n",
    "Urban sound recognition is a crucial component in the development of intelligent systems for cities, supporting applications in public safety, environmental monitoring, and transportation. This project focuses on designing and evaluating deep learning classifiers for recognizing urban sounds using the UrbanSound8K dataset. This dataset contains audio samples across 10 classes commonly found in urban environments, such as \"siren,\" \"dog bark,\" and \"street music.\" To address the classification challenge, we implement two neural network architectures: a **Recurrent Neural Network (RNN)**, which leverages temporal patterns in audio data, and a **Multilayer Perceptron (MLP)**, which classifies extracted features based on dense, fully connected layers. By comparing these models, we aim to identify which architecture is more effective in urban sound classification tasks, balancing accuracy, computational efficiency, and model complexity.\n",
    "\n",
    "\n",
    "### **Abstract**\n",
    "\n",
    "This project presents a deep learning approach for urban sound classification, utilizing the UrbanSound8K dataset to train and evaluate two neural network models: a Recurrent Neural Network (RNN) and a Multilayer Perceptron (MLP). Our data preparation pipeline applies comprehensive preprocessing steps, including signal normalization and feature extraction, to transform raw audio signals into structured representations suitable for deep learning input. Model architectures are iteratively optimized through careful adjustment of layer configurations, activation functions, and hyperparameters to enhance performance. \n",
    "Training strategies incorporate fine-tuning of optimizers, regularization techniques, and a robust 10-fold cross-validation approach to ensure reliable generalization and mitigate overfitting. Model performance is evaluated using classification accuracy and a cumulative confusion matrix across folds, providing insight into each model’s ability to handle distinct sound classes.\n",
    "\n",
    "Training strategies included fine-tuning of optimizers and regularization techniques, alongside a robust **10-fold cross-validation** to assess generalization. Classification performance is evaluated based on accuracy and the confusion matrix across folds, providing insights into each model's strengths in handling distinct sound classes. \n",
    "\n",
    "### **Objective of the work**\n",
    "The primary objective of this project is to develop and assess the efficacy of two distinct neural network architectures—RNN and MLP—for urban sound classification. By conducting a comparative analysis, we aim to provide understanding of each model's performance, identifying the trade-offs in accuracy, computational demands, and architectural complexity. \n",
    "\n",
    "### **Structure of the work**\n",
    "The project is structured into the following sections:\n",
    "1. **Data Preparation**: This section outlines the steps taken to preprocess the UrbanSound8K dataset\n",
    "2. **Model Architectures**: A detailed description of the RNN and MLP models...\n",
    "    - **RNN Architecture**: ...\n",
    "    - **MLP Architecture**: ...\n",
    "3. **Training Strategies**: This section discusses the training approaches used for both models...\n",
    "4. **Evaluation Metrics**: A description of the metrics used to evaluate model performance...\n",
    "5. **Results and Discussion**: A presentation of the results obtained from the models, including accuracy and ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## LIBRARIES ###############\n",
    "import soundata\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Downloading ['all', 'index']. Index is being stored in C:\\Users\\34660\\OneDrive - Universidad de Oviedo\\Ciencia e Ingeniería de Datos\\3º\\Deep Learning\\Project\\MCII-Project\\.venv\\Lib\\site-packages\\soundata\\datasets\\indexes, and the rest of files in C:\\Users\\34660\\Desktop\\DeepLearningAudios\n",
      "INFO: [all] downloading UrbanSound8K.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 43.1M/5.61G [00:18<57:04, 1.75MB/s]   "
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = soundata.initialize('urbansound8k', data_home=r'C:\\Users\\34660\\Desktop\\DeepLearningAudios')\n",
    "dataset.download()  # Dataset is downloaded to my directory\n",
    "dataset.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "This dataset's index must be downloaded. Did you run .download()?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\34660\\OneDrive - Universidad de Oviedo\\Ciencia e Ingeniería de Datos\\3º\\Deep Learning\\Project\\MCII-Project\\.venv\\lib\\site-packages\\soundata\\core.py:177\u001b[0m, in \u001b[0;36mDataset._index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fhandle:\n\u001b[0;32m    178\u001b[0m         index \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fhandle)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\34660\\\\OneDrive - Universidad de Oviedo\\\\Ciencia e Ingeniería de Datos\\\\3º\\\\Deep Learning\\\\Project\\\\MCII-Project\\\\.venv\\\\Lib\\\\site-packages\\\\soundata\\\\datasets\\\\indexes\\\\urbansound8k_index_1.0.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\34660\\OneDrive - Universidad de Oviedo\\Ciencia e Ingeniería de Datos\\3º\\Deep Learning\\Project\\MCII-Project\\.venv\\lib\\site-packages\\soundata\\core.py:391\u001b[0m, in \u001b[0;36mDataset.validate\u001b[1;34m(self, verbose)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate\u001b[39m(\u001b[38;5;28mself\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    380\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate if the stored dataset is a valid version\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \n\u001b[0;32m    382\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    388\u001b[0m \n\u001b[0;32m    389\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    390\u001b[0m     missing_files, invalid_checksums \u001b[38;5;241m=\u001b[39m validate\u001b[38;5;241m.\u001b[39mvalidator(\n\u001b[1;32m--> 391\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_home, verbose\u001b[38;5;241m=\u001b[39mverbose\n\u001b[0;32m    392\u001b[0m     )\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m missing_files, invalid_checksums\n",
      "File \u001b[1;32mc:\\Users\\34660\\OneDrive - Universidad de Oviedo\\Ciencia e Ingeniería de Datos\\3º\\Deep Learning\\Project\\MCII-Project\\.venv\\lib\\site-packages\\soundata\\core.py:47\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m---> 47\u001b[0m value \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\34660\\OneDrive - Universidad de Oviedo\\Ciencia e Ingeniería de Datos\\3º\\Deep Learning\\Project\\MCII-Project\\.venv\\lib\\site-packages\\soundata\\core.py:181\u001b[0m, in \u001b[0;36mDataset._index\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_data\u001b[38;5;241m.\u001b[39mremote:\n\u001b[1;32m--> 181\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    182\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms index must be downloaded. Did you run .download()?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    183\u001b[0m         )\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset index for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut not found. Make sure your sample indexes for testing are in soundata/tests/indexes/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    187\u001b[0m     )\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: This dataset's index must be downloaded. Did you run .download()?"
     ]
    }
   ],
   "source": [
    "dataset.validate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
