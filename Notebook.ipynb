{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Machine Learning II: Deep learning classifiers for urban sound data**\n",
    "<img src=\"images/portrait.png\" alt=\"Portrait\" style=\"width:100%; height:auto;\">\n",
    "\n",
    "===================================================================================================================\n",
    "\n",
    "\n",
    "## **Introduction**\n",
    "Urban sound recognition is a crucial component in the development of intelligent systems for cities, supporting applications in public safety, environmental monitoring, and transportation. This project focuses on designing and evaluating deep learning classifiers for recognizing urban sounds using the UrbanSound8K dataset. This dataset contains audio samples across 10 classes commonly found in urban environments, such as \"siren,\" \"dog bark,\" and \"street music.\" To address the classification challenge, we implement two neural network architectures: a **Recurrent Neural Network (RNN)**, which leverages temporal patterns in audio data, and a **Multilayer Perceptron (MLP)**, which classifies extracted features based on dense, fully connected layers. By comparing these models, we aim to identify which architecture is more effective in urban sound classification tasks, balancing accuracy, computational efficiency, and model complexity.\n",
    "\n",
    "\n",
    "### **Abstract**\n",
    "\n",
    "This project presents a deep learning approach for urban sound classification, utilizing the UrbanSound8K dataset to train and evaluate two neural network models: a Recurrent Neural Network (RNN) and a Multilayer Perceptron (MLP). Our data preparation pipeline applies comprehensive preprocessing steps, including signal normalization and feature extraction, to transform raw audio signals into structured representations suitable for deep learning input. Model architectures are iteratively optimized through careful adjustment of layer configurations, activation functions, and hyperparameters to enhance performance. \n",
    "Training strategies incorporate fine-tuning of optimizers, regularization techniques, and a robust 10-fold cross-validation approach to ensure reliable generalization and mitigate overfitting. Model performance is evaluated using classification accuracy and a cumulative confusion matrix across folds, providing insight into each model’s ability to handle distinct sound classes.\n",
    "\n",
    "Training strategies included fine-tuning of optimizers and regularization techniques, alongside a robust **10-fold cross-validation** to assess generalization. Classification performance is evaluated based on accuracy and the confusion matrix across folds, providing insights into each model's strengths in handling distinct sound classes. \n",
    "\n",
    "### **Objective of the work**\n",
    "The primary objective of this project is to develop and assess the efficacy of two distinct neural network architectures—RNN and MLP—for urban sound classification. By conducting a comparative analysis, we aim to provide understanding of each model's performance, identifying the trade-offs in accuracy, computational demands, and architectural complexity. \n",
    "\n",
    "### **Structure of the work**\n",
    "The project is structured into the following sections:\n",
    "1. **Data Preparation**: This section outlines the steps taken to preprocess the UrbanSound8K dataset\n",
    "2. **Model Architectures**: A detailed description of the RNN and MLP models...\n",
    "    - **RNN Architecture**: ...\n",
    "    - **MLP Architecture**: ...\n",
    "3. **Training Strategies**: This section discusses the training approaches used for both models...\n",
    "4. **Evaluation Metrics**: A description of the metrics used to evaluate model performance...\n",
    "5. **Results and Discussion**: A presentation of the results obtained from the models, including accuracy and ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## LIBRARIES ###############\n",
    "import soundata\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Downloading ['all', 'index']. Index is being stored in C:\\Users\\34660\\OneDrive - Universidad de Oviedo\\Ciencia e Ingeniería de Datos\\3º\\Deep Learning\\Project\\MCII-Project\\.venv\\Lib\\site-packages\\soundata\\datasets\\indexes, and the rest of files in C:\\Users\\34660\\Desktop\\DeepLearningAudios\n",
      "INFO: [all] downloading UrbanSound8K.tar.gz\n",
      "5.61GB [11:14, 8.94MB/s]                                \n",
      "INFO: [index] downloading urbansound8k_index_1.0.json\n",
      "1.15MB [00:00, 1.46MB/s]                            \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m soundata\u001b[38;5;241m.\u001b[39minitialize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murbansound8k\u001b[39m\u001b[38;5;124m'\u001b[39m, data_home\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m34660\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDeepLearningAudios\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m dataset\u001b[38;5;241m.\u001b[39mdownload(force_overwrite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Dataset is downloaded to my directory\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = soundata.initialize('urbansound8k', data_home=r'C:\\Users\\34660\\Desktop\\DeepLearningAudios')\n",
    "dataset.download(force_overwrite = True)  # Dataset is downloaded to my directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.58it/s]\n",
      "100%|██████████| 8732/8732 [01:38<00:00, 88.22it/s] \n",
      "INFO: Success: the dataset is complete and all files are valid.\n",
      "INFO: --------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'metadata': {}, 'clips': {}}, {'metadata': {}, 'clips': {}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.validate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
